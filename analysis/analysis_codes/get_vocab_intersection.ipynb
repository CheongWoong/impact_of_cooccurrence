{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-125m\n",
      "EleutherAI/gpt-neo-1.3B\n",
      "EleutherAI/gpt-neo-2.7B\n",
      "EleutherAI/gpt-j-6b\n",
      "bert-base-uncased\n",
      "bert-large-uncased\n",
      "roberta-base\n",
      "roberta-large\n",
      "albert-base-v1\n",
      "albert-large-v1\n",
      "albert-xlarge-v1\n",
      "albert-base-v2\n",
      "albert-large-v2\n",
      "albert-xlarge-v2\n"
     ]
    }
   ],
   "source": [
    "words = defaultdict(set)\n",
    "model_names = ['EleutherAI/gpt-neo-125m', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6b',\n",
    "                 'bert-base-uncased', 'bert-large-uncased',\n",
    "                 'roberta-base', 'roberta-large',\n",
    "                 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1',\n",
    "                 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2',\n",
    "                 ]\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    for word, i in tok.vocab.items():\n",
    "        new_word = tok.decode(i).strip().lower()\n",
    "        words[model_name].add(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32762\n",
      "32762\n",
      "32762\n",
      "32905\n",
      "30522\n",
      "30522\n",
      "32770\n",
      "32770\n",
      "26589\n",
      "26589\n",
      "26589\n",
      "26589\n",
      "26589\n",
      "26589\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(len(words[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_intersection = set()\n",
    "for vocab in words.values():\n",
    "    if len(vocab_intersection) == 0:\n",
    "        vocab_intersection = vocab\n",
    "    else:\n",
    "        vocab_intersection = vocab_intersection & vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16353"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "target_model = 'gpt-3.5-turbo'\n",
    "encoding = tiktoken.encoding_for_model(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_vocab = set()\n",
    "for i in range(encoding.n_vocab):\n",
    "    if i not in [100256, 100261, 100262, 100263, 100264, 100265, 100266, 100267, 100268, 100269,\n",
    "                 100270, 100271, 100272, 100273, 100274, 100275]:\n",
    "        w = encoding.decode([i])\n",
    "        chatgpt_vocab.add(w.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62643"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatgpt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab_intersection = vocab_intersection & chatgpt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab_intersection = list(final_vocab_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab_intersection_including_chatgpt.json', 'w') as fout:\n",
    "    json.dump(final_vocab_intersection, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
