{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average hits@1 for all models\n",
    "Set dataset_name, dataset_type, training_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    'bert-large-uncased': 'BERT$_{large}$',\n",
    "    'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    'roberta-base': 'RoBERTa$_{base}$',\n",
    "    'roberta-large': 'RoBERTa$_{large}$',\n",
    "    'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    'gpt-j-6b': 'GPT-J 6B',\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& BERT$_{base}$ & 0.08$\\pm$0.28 & 0.21$\\pm$0.40 & 0.38$\\pm$0.49 & 0.06$\\pm$0.05 & 0.20$\\pm$0.12 & 0.41$\\pm$0.15\\\\\n",
      "& BERT$_{large}$ & 0.07$\\pm$0.26 & 0.20$\\pm$0.40 & 0.38$\\pm$0.48 & 0.08$\\pm$0.09 & 0.21$\\pm$0.12 & 0.44$\\pm$0.15\\\\\n",
      "& ALBERT1$_{base}$ & 0.10$\\pm$0.30 & 0.22$\\pm$0.41 & 0.39$\\pm$0.49 & 0.05$\\pm$0.06 & 0.13$\\pm$0.11 & 0.33$\\pm$0.17\\\\\n",
      "& ALBERT1$_{large}$ & 0.08$\\pm$0.27 & 0.18$\\pm$0.38 & 0.34$\\pm$0.47 & 0.04$\\pm$0.04 & 0.12$\\pm$0.08 & 0.33$\\pm$0.15\\\\\n",
      "& ALBERT1$_{xlarge}$ & 0.05$\\pm$0.22 & 0.13$\\pm$0.33 & 0.27$\\pm$0.44 & 0.04$\\pm$0.05 & 0.11$\\pm$0.09 & 0.28$\\pm$0.15\\\\\n",
      "& ALBERT2$_{base}$ & 0.02$\\pm$0.14 & 0.09$\\pm$0.28 & 0.24$\\pm$0.43 & 0.02$\\pm$0.03 & 0.10$\\pm$0.07 & 0.32$\\pm$0.19\\\\\n",
      "& ALBERT2$_{large}$ & 0.11$\\pm$0.31 & 0.24$\\pm$0.43 & 0.40$\\pm$0.49 & 0.07$\\pm$0.07 & 0.21$\\pm$0.14 & 0.42$\\pm$0.17\\\\\n",
      "& ALBERT2$_{xlarge}$ & 0.08$\\pm$0.27 & 0.19$\\pm$0.40 & 0.36$\\pm$0.48 & 0.06$\\pm$0.06 & 0.19$\\pm$0.12 & 0.42$\\pm$0.21\\\\\n",
      "& RoBERTa$_{base}$ & 0.02$\\pm$0.14 & 0.08$\\pm$0.27 & 0.21$\\pm$0.41 & 0.03$\\pm$0.04 & 0.09$\\pm$0.08 & 0.25$\\pm$0.15\\\\\n",
      "& RoBERTa$_{large}$ & 0.03$\\pm$0.17 & 0.11$\\pm$0.31 & 0.24$\\pm$0.43 & 0.03$\\pm$0.05 & 0.10$\\pm$0.09 & 0.25$\\pm$0.17\\\\\n",
      "& GPT-Neo 125M & 0.02$\\pm$0.13 & 0.07$\\pm$0.25 & 0.20$\\pm$0.40 & 0.01$\\pm$0.02 & 0.06$\\pm$0.06 & 0.23$\\pm$0.14\\\\\n",
      "& GPT-Neo 1.3B & 0.02$\\pm$0.15 & 0.11$\\pm$0.31 & 0.31$\\pm$0.46 & 0.04$\\pm$0.09 & 0.13$\\pm$0.10 & 0.37$\\pm$0.18\\\\\n",
      "& GPT-Neo 2.7B & 0.03$\\pm$0.17 & 0.13$\\pm$0.34 & 0.36$\\pm$0.48 & 0.04$\\pm$0.09 & 0.16$\\pm$0.17 & 0.41$\\pm$0.18\\\\\n",
      "& GPT-J 6B & 0.05$\\pm$0.21 & 0.20$\\pm$0.40 & 0.45$\\pm$0.50 & 0.05$\\pm$0.06 & 0.22$\\pm$0.20 & 0.47$\\pm$0.20\\\\\n",
      "& ChatGPT-3.5 & 0.12$\\pm$0.32 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.12$\\pm$0.14 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "& ChatGPT-4 & 0.05$\\pm$0.21 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.05$\\pm$0.07 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_name_dict.keys():\n",
    "    try:\n",
    "        data = json.load(open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/score_{dataset_name}_{dataset_type}.json', 'r'))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    micro_hits_1 = data['hits@1_remove_stopwords'].split('+-')\n",
    "    micro_hits_10 = data['hits@10_remove_stopwords'].split('+-')\n",
    "    micro_hits_100 = data['hits@100_remove_stopwords'].split('+-')\n",
    "\n",
    "    relation_wise_hits_1, relation_wise_hits_10, relation_wise_hits_100 = [], [], []\n",
    "    for key in data:\n",
    "        if key.startswith('hits@1_remove_stopwords_'):\n",
    "            relation_wise_hits_1.append(float(data[key].split('+-')[0]))\n",
    "        elif key.startswith('hits@10_remove_stopwords_'):\n",
    "            relation_wise_hits_10.append(float(data[key].split('+-')[0]))\n",
    "        elif key.startswith('hits@100_remove_stopwords_'):\n",
    "            relation_wise_hits_100.append(float(data[key].split('+-')[0]))\n",
    "\n",
    "    macro_hits_1 = f'{np.mean(relation_wise_hits_1):.2f} +- {np.std(relation_wise_hits_1):.2f}'.split('+-')\n",
    "    macro_hits_10 = f'{np.mean(relation_wise_hits_10):.2f} +- {np.std(relation_wise_hits_10):.2f}'.split('+-')\n",
    "    macro_hits_100 = f'{np.mean(relation_wise_hits_100):.2f} +- {np.std(relation_wise_hits_100):.2f}'.split('+-')\n",
    "\n",
    "    line = f'& {model_name_dict[model_name]} ' + \\\n",
    "    f'& {micro_hits_1[0].strip()}$\\pm${micro_hits_1[1].strip()} ' + \\\n",
    "    f'& {micro_hits_10[0].strip()}$\\pm${micro_hits_10[1].strip()} ' + \\\n",
    "    f'& {micro_hits_100[0].strip()}$\\pm${micro_hits_100[1].strip()} ' + \\\n",
    "    f'& {macro_hits_1[0].strip()}$\\pm${macro_hits_1[1].strip()} ' + \\\n",
    "    f'& {macro_hits_10[0].strip()}$\\pm${macro_hits_10[1].strip()} ' + \\\n",
    "    f'& {macro_hits_100[0].strip()}$\\pm${macro_hits_100[1].strip()}\\\\\\\\'\n",
    "\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation-wise hits@1 for a pre-defined group of models\n",
    "Set dataset_name, dataset_type, training_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the group of model names for each run\n",
    "# model_names = ['bert-base-uncased', 'bert-large-uncased']\n",
    "# model_names = ['albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1']\n",
    "# model_names = ['albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2']\n",
    "# model_names = ['roberta-base', 'roberta-large']\n",
    "# model_names = ['gpt-neo-125m', 'gpt-neo-1.3B', 'gpt-neo-2.7B']\n",
    "model_names = ['gpt-j-6b', 'gpt-3.5-turbo-0125', 'gpt-4-0125-preview']\n",
    "\n",
    "# model_names = ['bert-base-uncased', 'bert-large-uncased']\n",
    "# model_names = ['gpt-neo-125m', 'gpt-j-6b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonym & 0.17$\\pm$0.38 & 0.38$\\pm$0.49 & 0.59$\\pm$0.49 & 0.45$\\pm$0.50 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.20$\\pm$0.40 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "AtLocation & 0.01$\\pm$0.08 & 0.05$\\pm$0.23 & 0.33$\\pm$0.47 & 0.03$\\pm$0.17 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.03$\\pm$0.16 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "CapableOf & 0.08$\\pm$0.28 & 0.24$\\pm$0.43 & 0.55$\\pm$0.50 & 0.14$\\pm$0.35 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.01$\\pm$0.08 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "Causes & 0.02$\\pm$0.15 & 0.09$\\pm$0.29 & 0.32$\\pm$0.46 & 0.03$\\pm$0.16 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.03$\\pm$0.18 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "CausesDesire & 0.08$\\pm$0.27 & 0.21$\\pm$0.41 & 0.59$\\pm$0.49 & 0.12$\\pm$0.33 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.05$\\pm$0.22 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "CreatedBy & 0.00$\\pm$0.00 & 0.19$\\pm$0.39 & 0.53$\\pm$0.50 & 0.16$\\pm$0.36 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "DefinedAs & 0.05$\\pm$0.22 & 0.11$\\pm$0.31 & 0.16$\\pm$0.36 & 0.08$\\pm$0.27 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "DerivedFrom & 0.07$\\pm$0.25 & 0.31$\\pm$0.46 & 0.71$\\pm$0.46 & 0.04$\\pm$0.19 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.14$\\pm$0.35 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "Desires & 0.05$\\pm$0.22 & 0.25$\\pm$0.43 & 0.56$\\pm$0.50 & 0.07$\\pm$0.26 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "DistinctFrom & 0.01$\\pm$0.08 & 0.04$\\pm$0.19 & 0.18$\\pm$0.38 & 0.10$\\pm$0.30 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "EtymologicallyDerivedFrom & 0.10$\\pm$0.30 & 0.20$\\pm$0.40 & 0.20$\\pm$0.40 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.33$\\pm$0.47 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "EtymologicallyRelatedTo & 0.02$\\pm$0.13 & 0.24$\\pm$0.43 & 0.54$\\pm$0.50 & 0.03$\\pm$0.16 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.06$\\pm$0.24 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "FormOf & 0.22$\\pm$0.41 & 0.73$\\pm$0.44 & 0.93$\\pm$0.26 & 0.73$\\pm$0.44 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.18$\\pm$0.38 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasA & 0.02$\\pm$0.15 & 0.11$\\pm$0.31 & 0.28$\\pm$0.45 & 0.12$\\pm$0.33 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasContext & 0.02$\\pm$0.13 & 0.08$\\pm$0.28 & 0.31$\\pm$0.46 & 0.16$\\pm$0.36 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.04 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasFirstSubevent & 0.08$\\pm$0.27 & 0.33$\\pm$0.47 & 0.68$\\pm$0.47 & 0.08$\\pm$0.27 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.08$\\pm$0.27 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasPrerequisite & 0.02$\\pm$0.13 & 0.09$\\pm$0.28 & 0.27$\\pm$0.44 & 0.05$\\pm$0.22 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.01$\\pm$0.10 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasProperty & 0.03$\\pm$0.16 & 0.10$\\pm$0.30 & 0.35$\\pm$0.48 & 0.06$\\pm$0.24 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.01$\\pm$0.09 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "HasSubevent & 0.04$\\pm$0.20 & 0.11$\\pm$0.32 & 0.40$\\pm$0.49 & 0.03$\\pm$0.18 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "IsA & 0.06$\\pm$0.23 & 0.28$\\pm$0.45 & 0.55$\\pm$0.50 & 0.09$\\pm$0.29 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.02$\\pm$0.15 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "LocatedNear & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.44$\\pm$0.50 & 0.14$\\pm$0.35 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "MadeOf & 0.21$\\pm$0.41 & 0.51$\\pm$0.50 & 0.80$\\pm$0.40 & 0.19$\\pm$0.39 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "MannerOf & 0.01$\\pm$0.10 & 0.13$\\pm$0.34 & 0.52$\\pm$0.50 & 0.12$\\pm$0.32 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.01$\\pm$0.10 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "MotivatedByGoal & 0.02$\\pm$0.15 & 0.15$\\pm$0.36 & 0.42$\\pm$0.49 & 0.03$\\pm$0.17 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.01$\\pm$0.10 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "NotCapableOf & 0.10$\\pm$0.30 & 0.20$\\pm$0.40 & 0.50$\\pm$0.50 & 0.33$\\pm$0.47 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.17$\\pm$0.37 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "NotDesires & 0.11$\\pm$0.31 & 0.17$\\pm$0.37 & 0.39$\\pm$0.49 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "NotHasProperty & 0.00$\\pm$0.00 & 0.03$\\pm$0.16 & 0.20$\\pm$0.40 & 0.03$\\pm$0.17 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "PartOf & 0.02$\\pm$0.15 & 0.18$\\pm$0.38 & 0.51$\\pm$0.50 & 0.20$\\pm$0.40 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.07$\\pm$0.25 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "RelatedTo & 0.04$\\pm$0.19 & 0.15$\\pm$0.36 & 0.38$\\pm$0.48 & 0.08$\\pm$0.27 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.04$\\pm$0.19 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "SimilarTo & 0.02$\\pm$0.12 & 0.09$\\pm$0.29 & 0.30$\\pm$0.46 & 0.15$\\pm$0.35 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.05$\\pm$0.21 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "SymbolOf & 0.00$\\pm$0.00 & 1.00$\\pm$0.00 & 1.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "Synonym & 0.05$\\pm$0.22 & 0.22$\\pm$0.41 & 0.48$\\pm$0.50 & 0.21$\\pm$0.41 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.09$\\pm$0.28 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "UsedFor & 0.04$\\pm$0.19 & 0.17$\\pm$0.37 & 0.45$\\pm$0.50 & 0.02$\\pm$0.14 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.02$\\pm$0.13 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n"
     ]
    }
   ],
   "source": [
    "relation_wise_line = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    data = json.load(open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/score_{dataset_name}_{dataset_type}.json', 'r'))\n",
    "\n",
    "    relation_wise_hits_1, relation_wise_hits_10, relation_wise_hits_100 = {}, {}, {}\n",
    "    for key in data:\n",
    "        if key.startswith('hits@1_remove_stopwords_'):\n",
    "            rel_id = key.split('hits@1_remove_stopwords_')[1]\n",
    "            relation_wise_hits_1[rel_id] = data[key].split('+-')\n",
    "        elif key.startswith('hits@10_remove_stopwords_'):\n",
    "            rel_id = key.split('hits@10_remove_stopwords_')[1]\n",
    "            relation_wise_hits_10[rel_id] = data[key].split('+-')\n",
    "        elif key.startswith('hits@100_remove_stopwords_'):\n",
    "            rel_id = key.split('hits@100_remove_stopwords_')[1]\n",
    "            relation_wise_hits_100[rel_id] = data[key].split('+-')\n",
    "\n",
    "    for rel_id in sorted(list(relation_wise_hits_1.keys())):\n",
    "        if rel_id not in relation_wise_line:\n",
    "            relation_wise_line[rel_id] = rel_id\n",
    "        line = f' & {relation_wise_hits_1[rel_id][0].strip()}$\\pm${relation_wise_hits_1[rel_id][1].strip()}' + \\\n",
    "        f' & {relation_wise_hits_10[rel_id][0].strip()}$\\pm${relation_wise_hits_10[rel_id][1].strip()}' + \\\n",
    "        f' & {relation_wise_hits_100[rel_id][0].strip()}$\\pm${relation_wise_hits_100[rel_id][1].strip()}'\n",
    "        \n",
    "        relation_wise_line[rel_id] += line\n",
    "\n",
    "for rel_id in relation_wise_line:\n",
    "    relation_wise_line[rel_id] += '\\\\\\\\'\n",
    "\n",
    "for line in relation_wise_line.values():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average hits@1 for term frequency baselines\n",
    "Set pretraining_dataset_name, dataset_name, dataset_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['marginal', 'joint', 'pmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_dataset_name = 'bert_pretraining_data'\n",
    "\n",
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& marginal & 0.00$\\pm$0.00 & 0.00$\\pm$0.01 & 0.00$\\pm$0.04 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00 & 0.00$\\pm$0.00\\\\\n",
      "& marginal & 0.00$\\pm$0.01 & 0.00$\\pm$0.02 & 0.01$\\pm$0.12 & 0.01$\\pm$0.06 & 0.05$\\pm$0.15 & 0.21$\\pm$0.35\\\\\n",
      "& marginal & 0.00$\\pm$0.01 & 0.01$\\pm$0.09 & 0.04$\\pm$0.19 & 0.02$\\pm$0.09 & 0.10$\\pm$0.21 & 0.42$\\pm$0.41\\\\\n",
      "& joint & 0.03$\\pm$0.17 & 0.14$\\pm$0.35 & 0.26$\\pm$0.44 & 0.02$\\pm$0.02 & 0.07$\\pm$0.09 & 0.19$\\pm$0.15\\\\\n",
      "& joint & 0.08$\\pm$0.27 & 0.20$\\pm$0.40 & 0.35$\\pm$0.48 & 0.09$\\pm$0.15 & 0.24$\\pm$0.22 & 0.49$\\pm$0.29\\\\\n",
      "& joint & 0.09$\\pm$0.29 & 0.23$\\pm$0.42 & 0.40$\\pm$0.49 & 0.14$\\pm$0.19 & 0.35$\\pm$0.23 & 0.68$\\pm$0.25\\\\\n",
      "& pmi & 0.02$\\pm$0.14 & 0.07$\\pm$0.25 & 0.13$\\pm$0.33 & 0.01$\\pm$0.01 & 0.06$\\pm$0.09 & 0.11$\\pm$0.09\\\\\n",
      "& pmi & 0.05$\\pm$0.21 & 0.10$\\pm$0.30 & 0.17$\\pm$0.37 & 0.05$\\pm$0.08 & 0.15$\\pm$0.16 & 0.35$\\pm$0.30\\\\\n",
      "& pmi & 0.05$\\pm$0.22 & 0.12$\\pm$0.32 & 0.22$\\pm$0.41 & 0.08$\\pm$0.10 & 0.22$\\pm$0.19 & 0.54$\\pm$0.32\\\\\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    try:\n",
    "        data = json.load(open(f'../../../results/{dataset_name}/{model_name}/{pretraining_dataset_name}/score_{dataset_name}_{dataset_type}.json', 'r'))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for type in ['remove_stopwords', 'gold_objs', 'gold_objs_relation_wise']:\n",
    "        micro_hits_1 = data[f'hits@1_{type}'].split('+-')\n",
    "        micro_hits_10 = data[f'hits@10_{type}'].split('+-')\n",
    "        micro_hits_100 = data[f'hits@100_{type}'].split('+-')\n",
    "\n",
    "        relation_wise_hits_1, relation_wise_hits_10, relation_wise_hits_100 = [], [], []\n",
    "        for key in data:\n",
    "            if key.startswith(f'hits@1_{type}_'):\n",
    "                relation_wise_hits_1.append(float(data[key].split('+-')[0]))\n",
    "            elif key.startswith(f'hits@10_{type}_'):\n",
    "                relation_wise_hits_10.append(float(data[key].split('+-')[0]))\n",
    "            elif key.startswith(f'hits@100_{type}_'):\n",
    "                relation_wise_hits_100.append(float(data[key].split('+-')[0]))\n",
    "\n",
    "        macro_hits_1 = f'{np.mean(relation_wise_hits_1):.2f} +- {np.std(relation_wise_hits_1):.2f}'.split('+-')\n",
    "        macro_hits_10 = f'{np.mean(relation_wise_hits_10):.2f} +- {np.std(relation_wise_hits_10):.2f}'.split('+-')\n",
    "        macro_hits_100 = f'{np.mean(relation_wise_hits_100):.2f} +- {np.std(relation_wise_hits_100):.2f}'.split('+-')\n",
    "\n",
    "        line = f'& {model_name} ' + \\\n",
    "        f'& {micro_hits_1[0].strip()}$\\pm${micro_hits_1[1].strip()} ' + \\\n",
    "        f'& {micro_hits_10[0].strip()}$\\pm${micro_hits_10[1].strip()} ' + \\\n",
    "        f'& {micro_hits_100[0].strip()}$\\pm${micro_hits_100[1].strip()} ' + \\\n",
    "        f'& {macro_hits_1[0].strip()}$\\pm${macro_hits_1[1].strip()} ' + \\\n",
    "        f'& {macro_hits_10[0].strip()}$\\pm${macro_hits_10[1].strip()} ' + \\\n",
    "        f'& {macro_hits_100[0].strip()}$\\pm${macro_hits_100[1].strip()}\\\\\\\\'\n",
    "\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
