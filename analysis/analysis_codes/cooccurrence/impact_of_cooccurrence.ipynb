{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheongwoong/miniconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cooccurrence_matrix import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_coo_matrix = CooccurrenceMatrix('pile')\n",
    "bert_coo_matrix = CooccurrenceMatrix('bert_pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "filter = {}\n",
    "for w in stopword_list:\n",
    "    filter[w] = w\n",
    "punctuations = {\n",
    "    \"?\": \"?\",\n",
    "    \":\": \":\",\n",
    "    \"!\": \"!\",\n",
    "    \".\": \".\",\n",
    "    \",\": \",\",\n",
    "    \";\": \";\"\n",
    "}\n",
    "filter.update(punctuations)\n",
    "def filtering(text):\n",
    "    if text in filter:\n",
    "        return True\n",
    "\n",
    "def text_normalization_without_lemmatization(text):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if filtering(token_low):\n",
    "            continue\n",
    "        result.append(token_low)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'LAMA_TREx'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/{dataset_name}/all.json\", 'r') as fin:\n",
    "    f_all = json.load(fin)\n",
    "\n",
    "uid_rel_map, uid_subj_map, uid_obj_map = {}, {}, {}\n",
    "for example in f_all:\n",
    "    uid_subj_map[example['uid']] = example['subj']\n",
    "    uid_rel_map[example['uid']] = example['rel_id']\n",
    "    uid_obj_map[example['uid']] = example['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sections = 10\n",
    "\n",
    "def prob_value_to_section(value):\n",
    "    return min(int(np.ceil(-np.log10(value+0.000001))), num_sections - 1)\n",
    "\n",
    "def prob_section_to_string(section):\n",
    "        denominator = str(10**section) if section < num_sections - 1 else 'inf'\n",
    "        return '1/'+denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "==============================\n",
      "Model: gpt-neo-125m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8824it [00:04, 2150.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 3, 3: 119, 4: 707, 5: 1352, 6: 6114, 7: 0, 8: 0, 9: 0}\n",
      "{\n",
      "    \"hits@1_remove_stopwords_section_1/1\": \"nan +- nan\",\n",
      "    \"hits@1_remove_stopwords_section_1/10\": \"nan +- nan\",\n",
      "    \"hits@1_remove_stopwords_section_1/100\": \"0.00 +- 0.00\",\n",
      "    \"hits@1_remove_stopwords_section_1/1000\": \"0.17 +- 0.37\",\n",
      "    \"hits@1_remove_stopwords_section_1/10000\": \"0.09 +- 0.28\",\n",
      "    \"hits@1_remove_stopwords_section_1/100000\": \"0.15 +- 0.36\",\n",
      "    \"hits@1_remove_stopwords_section_1/1000000\": \"0.10 +- 0.30\",\n",
      "    \"hits@1_remove_stopwords_section_1/10000000\": \"nan +- nan\",\n",
      "    \"hits@1_remove_stopwords_section_1/100000000\": \"nan +- nan\",\n",
      "    \"hits@1_remove_stopwords_section_1/inf\": \"nan +- nan\",\n",
      "    \"hits@100_remove_stopwords_section_1/1\": \"nan +- nan\",\n",
      "    \"hits@100_remove_stopwords_section_1/10\": \"nan +- nan\",\n",
      "    \"hits@100_remove_stopwords_section_1/100\": \"1.00 +- 0.00\",\n",
      "    \"hits@100_remove_stopwords_section_1/1000\": \"0.82 +- 0.39\",\n",
      "    \"hits@100_remove_stopwords_section_1/10000\": \"0.77 +- 0.42\",\n",
      "    \"hits@100_remove_stopwords_section_1/100000\": \"0.62 +- 0.49\",\n",
      "    \"hits@100_remove_stopwords_section_1/1000000\": \"0.49 +- 0.50\",\n",
      "    \"hits@100_remove_stopwords_section_1/10000000\": \"nan +- nan\",\n",
      "    \"hits@100_remove_stopwords_section_1/100000000\": \"nan +- nan\",\n",
      "    \"hits@100_remove_stopwords_section_1/inf\": \"nan +- nan\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT%_{base}%',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4-0125' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        prob = joint_prob\n",
    "\n",
    "        section = prob_value_to_section(prob)\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        if section not in rel_results_hits_1[rel]:\n",
    "            rel_results_hits_1[rel][section] = []\n",
    "            rel_results_hits_10[rel][section] = []\n",
    "            rel_results_hits_100[rel][section] = []\n",
    "        rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "            rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(num_sections)\n",
    "    sorted_rels = sorted(list(rel_results_hits_1.keys()))\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "        if section in results_hits_1:\n",
    "            results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "            results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "            results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "        # for rel in rel_results_hits_1:\n",
    "        #     if section in rel_results_hits_1[rel]:\n",
    "        #         rel_results_hits_1[rel][section] = np.mean(rel_results_hits_1[rel][section]), np.std(rel_results_hits_1[rel][section])\n",
    "        #         rel_results_hits_10[rel][section] = np.mean(rel_results_hits_10[rel][section]), np.std(rel_results_hits_10[rel][section])\n",
    "        #         rel_results_hits_100[rel][section] = np.mean(rel_results_hits_100[rel][section]), np.std(rel_results_hits_100[rel][section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_hits_1:\n",
    "            result[f'hits@1_remove_stopwords_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "    \n",
    "    # for section in sections:\n",
    "    #     if section in results_hits_10:\n",
    "    #         result[f'hits@10_remove_stopwords_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_10[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_100:\n",
    "            result[f'hits@100_remove_stopwords_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    # for section in sections:\n",
    "    #     for rel in sorted_rels:\n",
    "    #         if section in rel_results_hits_1[rel]:\n",
    "    #             result[f'hits_1_remove_stopwords_{rel}_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % rel_results_hits_1[rel][section]\n",
    "\n",
    "    # for section in sections:\n",
    "    #     for rel in sorted_rels:\n",
    "    #         if section in rel_results_hits_10[rel]:\n",
    "    #             result[f'hits_10_remove_stopwords_{rel}_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % rel_results_hits_10[rel][section]\n",
    "\n",
    "    # for section in sections:\n",
    "    #     for rel in sorted_rels:\n",
    "    #         if section in rel_results_hits_100[rel]:\n",
    "    #             result[f'hits_100_remove_stopwords_{rel}_section_{prob_section_to_string(section)}'] = f'%.2f +- %.2f' % rel_results_hits_100[rel][section]\n",
    "    \n",
    "    print(num_samples)\n",
    "    print(json.dumps(result, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
