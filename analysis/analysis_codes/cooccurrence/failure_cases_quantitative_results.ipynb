{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cooccurrence_matrix import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_coo_matrix = CooccurrenceMatrix('pile')\n",
    "bert_coo_matrix = CooccurrenceMatrix('bert_pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "filter = {}\n",
    "for w in stopword_list:\n",
    "    filter[w] = w\n",
    "punctuations = {\n",
    "    \"?\": \"?\",\n",
    "    \":\": \":\",\n",
    "    \"!\": \"!\",\n",
    "    \".\": \".\",\n",
    "    \",\": \",\",\n",
    "    \";\": \";\"\n",
    "}\n",
    "filter.update(punctuations)\n",
    "def filtering(text):\n",
    "    if text in filter:\n",
    "        return True\n",
    "\n",
    "def text_normalization_without_lemmatization(text):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if filtering(token_low):\n",
    "            continue\n",
    "        result.append(token_low)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    # 'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/{dataset_name}/all.json\", 'r') as fin:\n",
    "    f_all = json.load(fin)\n",
    "\n",
    "uid_rel_map = {}\n",
    "uid_subj_map = {}\n",
    "rel_subj_objects = defaultdict(set)\n",
    "for example in f_all:\n",
    "    subj = example['subj']\n",
    "    rel = example['rel_id']\n",
    "    obj = example['output']\n",
    "\n",
    "    uid_subj_map[example['uid']] = subj\n",
    "    uid_rel_map[example['uid']] = rel\n",
    "    rel_subj_objects[rel+'_'+subj].add(obj.lower())\n",
    "for key in rel_subj_objects:\n",
    "    rel_subj_objects[key] = list(rel_subj_objects[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sections = 8\n",
    "def prob_value_to_section(value):\n",
    "    return min(int(np.ceil(-np.log2(value+0.000001))), num_sections - 1)\n",
    "    \n",
    "for model_name in model_name_dict.keys():\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    condprob_gt_bin_total = defaultdict(list)\n",
    "    condprob_pred_bin_total = defaultdict(list)\n",
    "    condprob_gt_bin_success = defaultdict(list)\n",
    "    condprob_pred_bin_success = defaultdict(list)\n",
    "    condprob_gt_bin_failure = defaultdict(list)\n",
    "    condprob_pred_bin_failure = defaultdict(list)\n",
    "\n",
    "    count_bin_failure = defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        label_text = pred['label_text'].lower()\n",
    "        rel_subj_object = deepcopy(rel_subj_objects[rel+'_'+subj])\n",
    "        rel_subj_object.remove(label_text)\n",
    "\n",
    "        if 'top_100_text_remove_stopwords' in pred:\n",
    "            pred_top_k_remove_stopwords = pred['top_100_text_remove_stopwords']\n",
    "        else:\n",
    "            pred_top_k_remove_stopwords = pred['top_5_text_remove_stopwords']\n",
    "        \n",
    "        # we remove other valid objects for a subject-relation pair other than the one we test\n",
    "        for w in pred_top_k_remove_stopwords:\n",
    "            w = w.lower().strip()\n",
    "            if w not in rel_subj_object or True:\n",
    "                pred_top_1_remove_stopwords = w\n",
    "                break\n",
    "\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj_gt = ' '.join(text_normalization_without_lemmatization(label_text))\n",
    "        obj_pred = ' '.join(text_normalization_without_lemmatization(pred_top_1_remove_stopwords))\n",
    "        joint_freq_gt = coo_matrix.coo_count(subj, obj_gt)\n",
    "        joint_freq_pred = coo_matrix.coo_count(subj, obj_pred)\n",
    "        \n",
    "        subj_freq = coo_matrix.count(subj)\n",
    "        # skip if the entities are composed of more than 3 tokens, or are stopwords\n",
    "        if joint_freq_gt <= 0 or joint_freq_pred <= 0 or subj_freq <= 0:\n",
    "            continue\n",
    "        cond_prob_gt = joint_freq_gt / subj_freq if subj_freq > 0 else 0\n",
    "        cond_prob_pred = joint_freq_pred / subj_freq if subj_freq > 0 else 0\n",
    "\n",
    "        bin = prob_value_to_section(cond_prob_gt)\n",
    "\n",
    "        condprob_gt_bin_total[bin].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total[bin].append(cond_prob_pred)\n",
    "        condprob_gt_bin_total['total'].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total['total'].append(cond_prob_pred)\n",
    "        if pred['hits@1_remove_stopwords'] > 0.5:\n",
    "            condprob_gt_bin_success[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_success['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success['total'].append(cond_prob_pred)\n",
    "        else:\n",
    "            condprob_gt_bin_failure[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_failure['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure['total'].append(cond_prob_pred)\n",
    "            count_bin_failure[bin].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "            count_bin_failure['total'].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "\n",
    "    # print('Total')\n",
    "    # for bin in ['total'] + list(range(num_sections)):\n",
    "    #     print(f\"{bin} / {round(np.mean(condprob_pred_bin_total[bin]), 2)} +- {round(np.std(condprob_pred_bin_total[bin]), 2) } / {round(np.mean(condprob_gt_bin_total[bin]), 2)} +- {round(np.std(condprob_gt_bin_total[bin]), 2)} / {len(condprob_pred_bin_total[bin])}\")\n",
    "    print('Count in failure cases')\n",
    "    for bin in ['total'] + list(range(num_sections)):\n",
    "        try:\n",
    "            print(f\"{bin} / {int(np.mean(count_bin_failure[bin])*100)}% / {len(count_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    print('Failure cases')\n",
    "    for bin in ['total'] + list(range(num_sections)):\n",
    "        try:\n",
    "            print(f\"{bin} / {round(np.mean(condprob_pred_bin_failure[bin]), 2)} +- {round(np.std(condprob_pred_bin_failure[bin]), 2) } / {round(np.mean(condprob_gt_bin_failure[bin]), 2)} +- {round(np.std(condprob_gt_bin_failure[bin]), 2)} / {len(condprob_gt_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'prompt_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sections = 8\n",
    "def prob_value_to_section(value):\n",
    "    return min(int(np.ceil(-np.log2(value+0.000001))), num_sections - 1)\n",
    "    \n",
    "for model_name in model_name_dict.keys():\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    condprob_gt_bin_total = defaultdict(list)\n",
    "    condprob_pred_bin_total = defaultdict(list)\n",
    "    condprob_gt_bin_success = defaultdict(list)\n",
    "    condprob_pred_bin_success = defaultdict(list)\n",
    "    condprob_gt_bin_failure = defaultdict(list)\n",
    "    condprob_pred_bin_failure = defaultdict(list)\n",
    "\n",
    "    count_bin_failure = defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        label_text = pred['label_text'].lower()\n",
    "        rel_subj_object = deepcopy(rel_subj_objects[rel+'_'+subj])\n",
    "        rel_subj_object.remove(label_text)\n",
    "\n",
    "        if 'top_100_text_remove_stopwords' in pred:\n",
    "            pred_top_k_remove_stopwords = pred['top_100_text_remove_stopwords']\n",
    "        else:\n",
    "            pred_top_k_remove_stopwords = pred['top_5_text_remove_stopwords']\n",
    "        \n",
    "        # we remove other valid objects for a subject-relation pair other than the one we test\n",
    "        for w in pred_top_k_remove_stopwords:\n",
    "            w = w.lower().strip()\n",
    "            if w not in rel_subj_object or True:\n",
    "                pred_top_1_remove_stopwords = w\n",
    "                break\n",
    "\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj_gt = ' '.join(text_normalization_without_lemmatization(label_text))\n",
    "        obj_pred = ' '.join(text_normalization_without_lemmatization(pred_top_1_remove_stopwords))\n",
    "        joint_freq_gt = coo_matrix.coo_count(subj, obj_gt)\n",
    "        joint_freq_pred = coo_matrix.coo_count(subj, obj_pred)\n",
    "        \n",
    "        subj_freq = coo_matrix.count(subj)\n",
    "        # skip if the entities are composed of more than 3 tokens, or are stopwords\n",
    "        if joint_freq_gt <= 0 or joint_freq_pred <= 0 or subj_freq <= 0:\n",
    "            continue\n",
    "        cond_prob_gt = joint_freq_gt / subj_freq if subj_freq > 0 else 0\n",
    "        cond_prob_pred = joint_freq_pred / subj_freq if subj_freq > 0 else 0\n",
    "\n",
    "        bin = prob_value_to_section(cond_prob_gt)\n",
    "\n",
    "        condprob_gt_bin_total[bin].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total[bin].append(cond_prob_pred)\n",
    "        condprob_gt_bin_total['total'].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total['total'].append(cond_prob_pred)\n",
    "        if pred['hits@1_remove_stopwords'] > 0.5:\n",
    "            condprob_gt_bin_success[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_success['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success['total'].append(cond_prob_pred)\n",
    "        else:\n",
    "            condprob_gt_bin_failure[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_failure['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure['total'].append(cond_prob_pred)\n",
    "            count_bin_failure[bin].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "            count_bin_failure['total'].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "\n",
    "    # print('Total')\n",
    "    # for bin in ['total'] + list(range(num_sections)):\n",
    "    #     print(f\"{bin} / {round(np.mean(condprob_pred_bin_total[bin]), 2)} +- {round(np.std(condprob_pred_bin_total[bin]), 2) } / {round(np.mean(condprob_gt_bin_total[bin]), 2)} +- {round(np.std(condprob_gt_bin_total[bin]), 2)} / {len(condprob_pred_bin_total[bin])}\")\n",
    "    print('Count in failure cases')\n",
    "    for bin in ['total'] + list(range(num_sections)):\n",
    "        try:\n",
    "            print(f\"{bin} / {int(np.mean(count_bin_failure[bin])*100)}% / {len(count_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    print('Failure cases')\n",
    "    for bin in ['total'] + list(range(num_sections)):\n",
    "        try:\n",
    "            print(f\"{bin} / {round(np.mean(condprob_pred_bin_failure[bin]), 2)} +- {round(np.std(condprob_pred_bin_failure[bin]), 2) } / {round(np.mean(condprob_gt_bin_failure[bin]), 2)} +- {round(np.std(condprob_gt_bin_failure[bin]), 2)} / {len(condprob_gt_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
