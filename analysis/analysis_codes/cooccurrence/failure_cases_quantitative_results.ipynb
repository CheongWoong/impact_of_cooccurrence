{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cooccurrence_matrix import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_coo_matrix = CooccurrenceMatrix('pile')\n",
    "bert_coo_matrix = CooccurrenceMatrix('bert_pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "filter = {}\n",
    "for w in stopword_list:\n",
    "    filter[w] = w\n",
    "punctuations = {\n",
    "    \"?\": \"?\",\n",
    "    \":\": \":\",\n",
    "    \"!\": \"!\",\n",
    "    \".\": \".\",\n",
    "    \",\": \",\",\n",
    "    \";\": \";\"\n",
    "}\n",
    "filter.update(punctuations)\n",
    "def filtering(text):\n",
    "    if text in filter:\n",
    "        return True\n",
    "\n",
    "def text_normalization_without_lemmatization(text):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if filtering(token_low):\n",
    "            continue\n",
    "        result.append(token_low)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'Meta-Llama-3-8B': 'Llama-3 8B',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/{dataset_name}/all.json\", 'r') as fin:\n",
    "    f_all = json.load(fin)\n",
    "\n",
    "uid_rel_map = {}\n",
    "uid_subj_map = {}\n",
    "rel_subj_objects = defaultdict(set)\n",
    "for example in f_all:\n",
    "    subj = example['subj']\n",
    "    rel = example['rel_id']\n",
    "    obj = example['output']\n",
    "\n",
    "    uid_subj_map[example['uid']] = subj\n",
    "    uid_rel_map[example['uid']] = rel\n",
    "    rel_subj_objects[rel+'_'+subj].add(obj.lower())\n",
    "for key in rel_subj_objects:\n",
    "    rel_subj_objects[key] = list(rel_subj_objects[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 1/10, 1/100, 1/1000, 1/10000]\n",
    "# bins = [1, 1/10, 1/100, 1/1000]\n",
    "\n",
    "def frequency_to_section(value):\n",
    "    return np.digitize(value, bins, right=True)\n",
    "\n",
    "def frequency_section_to_string(section):\n",
    "    return f'{section}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "for model_name in model_name_dict.keys():\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    condprob_gt_bin_total = defaultdict(list)\n",
    "    condprob_pred_bin_total = defaultdict(list)\n",
    "    condprob_gt_bin_success = defaultdict(list)\n",
    "    condprob_pred_bin_success = defaultdict(list)\n",
    "    condprob_gt_bin_failure = defaultdict(list)\n",
    "    condprob_pred_bin_failure = defaultdict(list)\n",
    "\n",
    "    count_bin_failure = defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        label_text = pred['label_text'].lower()\n",
    "        rel_subj_object = deepcopy(rel_subj_objects[rel+'_'+subj])\n",
    "        rel_subj_object.remove(label_text)\n",
    "\n",
    "        if 'top_100_text_remove_stopwords' in pred:\n",
    "            pred_top_k_remove_stopwords = pred['top_100_text_remove_stopwords']\n",
    "        else:\n",
    "            pred_top_k_remove_stopwords = pred['top_k_text_remove_stopwords']\n",
    "        \n",
    "        # we remove other valid objects for a subject-relation pair other than the one we test\n",
    "        for w in pred_top_k_remove_stopwords:\n",
    "            w = w.lower().strip()\n",
    "            if w not in rel_subj_object or True:\n",
    "                pred_top_1_remove_stopwords = w\n",
    "                break\n",
    "\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj_gt = ' '.join(text_normalization_without_lemmatization(label_text))\n",
    "        obj_pred = ' '.join(text_normalization_without_lemmatization(pred_top_1_remove_stopwords))\n",
    "        joint_freq_gt = coo_matrix.coo_count(subj, obj_gt)\n",
    "        joint_freq_pred = coo_matrix.coo_count(subj, obj_pred)\n",
    "        \n",
    "        subj_freq = coo_matrix.count(subj)\n",
    "        # skip if the entities are composed of more than 3 tokens, or are stopwords\n",
    "        if joint_freq_gt <= 0 or joint_freq_pred <= 0 or subj_freq <= 0:\n",
    "            continue\n",
    "        cond_prob_gt = joint_freq_gt / subj_freq if subj_freq > 0 else 0\n",
    "        cond_prob_pred = joint_freq_pred / subj_freq if subj_freq > 0 else 0\n",
    "\n",
    "        bin = frequency_to_section(cond_prob_gt)\n",
    "\n",
    "        condprob_gt_bin_total[bin].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total[bin].append(cond_prob_pred)\n",
    "        condprob_gt_bin_total['total'].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total['total'].append(cond_prob_pred)\n",
    "        if pred['hits@1_remove_stopwords'] > 0.5:\n",
    "            condprob_gt_bin_success[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_success['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success['total'].append(cond_prob_pred)\n",
    "        else:\n",
    "            condprob_gt_bin_failure[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_failure['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure['total'].append(cond_prob_pred)\n",
    "            count_bin_failure[bin].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "            count_bin_failure['total'].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "\n",
    "    num_sections = 5\n",
    "    # print('Total')\n",
    "    # for bin in ['total'] + list(range(num_sections)):\n",
    "    #     print(f\"{bin} / {round(np.mean(condprob_pred_bin_total[bin]), 2)} +- {round(np.std(condprob_pred_bin_total[bin]), 2) } / {round(np.mean(condprob_gt_bin_total[bin]), 2)} +- {round(np.std(condprob_gt_bin_total[bin]), 2)} / {len(condprob_pred_bin_total[bin])}\")\n",
    "    print('Count in failure cases')\n",
    "    for bin in ['total'] + list(range(1, num_sections+1)):\n",
    "        try:\n",
    "            print(f\"{bin} / {int(np.mean(count_bin_failure[bin])*100)}% / {len(count_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    print('Failure cases')\n",
    "    for bin in ['total'] + list(range(1, num_sections+1)):\n",
    "        try:\n",
    "            print(f\"{bin} / {round(np.mean(condprob_pred_bin_failure[bin]), 2)} +- {round(np.std(condprob_pred_bin_failure[bin]), 2) } / {round(np.mean(condprob_gt_bin_failure[bin]), 2)} +- {round(np.std(condprob_gt_bin_failure[bin]), 2)} / {len(condprob_gt_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'prompt_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    condprob_gt_bin_total = defaultdict(list)\n",
    "    condprob_pred_bin_total = defaultdict(list)\n",
    "    condprob_gt_bin_success = defaultdict(list)\n",
    "    condprob_pred_bin_success = defaultdict(list)\n",
    "    condprob_gt_bin_failure = defaultdict(list)\n",
    "    condprob_pred_bin_failure = defaultdict(list)\n",
    "\n",
    "    count_bin_failure = defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        label_text = pred['label_text'].lower()\n",
    "        rel_subj_object = deepcopy(rel_subj_objects[rel+'_'+subj])\n",
    "        rel_subj_object.remove(label_text)\n",
    "\n",
    "        if 'top_100_text_remove_stopwords' in pred:\n",
    "            pred_top_k_remove_stopwords = pred['top_100_text_remove_stopwords']\n",
    "        else:\n",
    "            pred_top_k_remove_stopwords = pred['top_k_text_remove_stopwords']\n",
    "        \n",
    "        # we remove other valid objects for a subject-relation pair other than the one we test\n",
    "        for w in pred_top_k_remove_stopwords:\n",
    "            w = w.lower().strip()\n",
    "            if w not in rel_subj_object or True:\n",
    "                pred_top_1_remove_stopwords = w\n",
    "                break\n",
    "\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj_gt = ' '.join(text_normalization_without_lemmatization(label_text))\n",
    "        obj_pred = ' '.join(text_normalization_without_lemmatization(pred_top_1_remove_stopwords))\n",
    "        joint_freq_gt = coo_matrix.coo_count(subj, obj_gt)\n",
    "        joint_freq_pred = coo_matrix.coo_count(subj, obj_pred)\n",
    "        \n",
    "        subj_freq = coo_matrix.count(subj)\n",
    "        # skip if the entities are composed of more than 3 tokens, or are stopwords\n",
    "        if joint_freq_gt <= 0 or joint_freq_pred <= 0 or subj_freq <= 0:\n",
    "            continue\n",
    "        cond_prob_gt = joint_freq_gt / subj_freq if subj_freq > 0 else 0\n",
    "        cond_prob_pred = joint_freq_pred / subj_freq if subj_freq > 0 else 0\n",
    "\n",
    "        bin = frequency_to_section(cond_prob_gt)\n",
    "\n",
    "        condprob_gt_bin_total[bin].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total[bin].append(cond_prob_pred)\n",
    "        condprob_gt_bin_total['total'].append(cond_prob_gt)\n",
    "        condprob_pred_bin_total['total'].append(cond_prob_pred)\n",
    "        if pred['hits@1_remove_stopwords'] > 0.5:\n",
    "            condprob_gt_bin_success[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_success['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_success['total'].append(cond_prob_pred)\n",
    "        else:\n",
    "            condprob_gt_bin_failure[bin].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure[bin].append(cond_prob_pred)\n",
    "            condprob_gt_bin_failure['total'].append(cond_prob_gt)\n",
    "            condprob_pred_bin_failure['total'].append(cond_prob_pred)\n",
    "            count_bin_failure[bin].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "            count_bin_failure['total'].append((cond_prob_pred > cond_prob_gt)*1)\n",
    "\n",
    "    num_sections = 5\n",
    "    # print('Total')\n",
    "    # for bin in ['total'] + list(range(num_sections)):\n",
    "    #     print(f\"{bin} / {round(np.mean(condprob_pred_bin_total[bin]), 2)} +- {round(np.std(condprob_pred_bin_total[bin]), 2) } / {round(np.mean(condprob_gt_bin_total[bin]), 2)} +- {round(np.std(condprob_gt_bin_total[bin]), 2)} / {len(condprob_pred_bin_total[bin])}\")\n",
    "    print('Count in failure cases')\n",
    "    for bin in ['total'] + list(range(1, num_sections+1)):\n",
    "        try:\n",
    "            print(f\"{bin} / {int(np.mean(count_bin_failure[bin])*100)}% / {len(count_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    print('Failure cases')\n",
    "    for bin in ['total'] + list(range(1, num_sections+1)):\n",
    "        try:\n",
    "            print(f\"{bin} / {round(np.mean(condprob_pred_bin_failure[bin]), 2)} +- {round(np.std(condprob_pred_bin_failure[bin]), 2) } / {round(np.mean(condprob_gt_bin_failure[bin]), 2)} +- {round(np.std(condprob_gt_bin_failure[bin]), 2)} / {len(condprob_gt_bin_failure[bin])}\")\n",
    "        except:\n",
    "            print(bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
