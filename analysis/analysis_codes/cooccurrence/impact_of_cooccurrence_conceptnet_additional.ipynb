{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cooccurrence_matrix import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_coo_matrix = CooccurrenceMatrix('pile')\n",
    "bert_coo_matrix = CooccurrenceMatrix('bert_pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "filter = {}\n",
    "for w in stopword_list:\n",
    "    filter[w] = w\n",
    "punctuations = {\n",
    "    \"?\": \"?\",\n",
    "    \":\": \":\",\n",
    "    \"!\": \"!\",\n",
    "    \".\": \".\",\n",
    "    \",\": \",\",\n",
    "    \";\": \";\"\n",
    "}\n",
    "filter.update(punctuations)\n",
    "def filtering(text):\n",
    "    if text in filter:\n",
    "        return True\n",
    "\n",
    "def text_normalization_without_lemmatization(text):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if filtering(token_low):\n",
    "            continue\n",
    "        result.append(token_low)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ConceptNet'\n",
    "dataset_type = 'test'\n",
    "\n",
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../../data/{dataset_name}/all.json\", 'r') as fin:\n",
    "    f_all = json.load(fin)\n",
    "\n",
    "uid_rel_map, uid_subj_map, uid_obj_map = {}, {}, {}\n",
    "for example in f_all:\n",
    "    uid_subj_map[example['uid']] = example['subj']\n",
    "    uid_rel_map[example['uid']] = example['rel_id']\n",
    "    uid_obj_map[example['uid']] = example['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "def frequency_to_section(value):\n",
    "    return np.digitize(value, bins)\n",
    "\n",
    "def frequency_section_to_string(section):\n",
    "    return f'{section}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    'Meta-Llama-3-8B-Instruct': 'Llama-3 8B Instruct',\n",
    "}\n",
    "\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:orange']\n",
    "\n",
    "markers = ['o', '^', 's', 'D']\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, model_name in enumerate(model_name_dict.keys()):\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        freq = subj_obj_count\n",
    "        section = frequency_to_section(freq)\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    sorted_rels = sorted(list(rel_results_hits_1.keys()))\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "        if section in results_hits_1:\n",
    "            results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "            results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "            results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_hits_1:\n",
    "            result[f'hits@1_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_100:\n",
    "            result[f'hits@100_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_100_mean = [results_hits_100[section][0] for section in sections]\n",
    "    hits_100_std = [results_hits_100[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[i], color=colors[i], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Hits@100', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'{dataset_name}_{dataset_type}_{training_type}_hits@100_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    'gpt-4o-2024-08-06': 'ChatGPT-4o',\n",
    "}\n",
    "\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "\n",
    "markers = ['o', '^', 's']\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, model_name in enumerate(model_name_dict.keys()):\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        freq = subj_obj_count\n",
    "        section = frequency_to_section(freq)\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    sorted_rels = sorted(list(rel_results_hits_1.keys()))\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "        if section in results_hits_1:\n",
    "            results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "            results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "            results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_hits_1:\n",
    "            result[f'hits@1_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_10:\n",
    "            result[f'hits@10_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_10[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_100:\n",
    "            result[f'hits@100_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_10_mean = [results_hits_10[section][0] for section in sections]\n",
    "    hits_10_std = [results_hits_10[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_10_mean, marker=markers[i], color=colors[i], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Hits@10', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'{dataset_name}_{dataset_type}_{training_type}_hits@10_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    'gpt-4o-2024-08-06': 'ChatGPT-4o',\n",
    "}\n",
    "\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "\n",
    "markers = ['o', '^', 's']\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, model_name in enumerate(model_name_dict.keys()):\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}_4_shot.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        freq = subj_obj_count\n",
    "        section = frequency_to_section(freq)\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    sorted_rels = sorted(list(rel_results_hits_1.keys()))\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "        if section in results_hits_1:\n",
    "            results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "            results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "            results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_hits_1:\n",
    "            result[f'hits@1_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_10:\n",
    "            result[f'hits@10_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_10[section]\n",
    "\n",
    "    for section in sections:\n",
    "        if section in results_hits_100:\n",
    "            result[f'hits@100_remove_stopwords_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_10_mean = [results_hits_10[section][0] for section in sections]\n",
    "    hits_10_std = [results_hits_10[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_10_mean, marker=markers[i], color=colors[i], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Hits@10', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'{dataset_name}_{dataset_type}_4shot_hits@10_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model_name_dict = {\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "joint_freq_bins = [f'$10^{i}$' for i in range(6+1)]\n",
    "subject_freq_bins = [f'$10^{i+1}$' for i in range(6)]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        joint_freq = subj_obj_count\n",
    "        joint_section = frequency_to_section(joint_freq)\n",
    "\n",
    "        subj_freq = subj_count\n",
    "        subj_section = frequency_to_section(subj_freq)\n",
    "\n",
    "        section = f'{joint_section}_{subj_section}'\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    joint_sections = range(1, len(bins)+1)\n",
    "    subj_sections = range(1, len(bins)+1)\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "            if section in results_hits_1:\n",
    "                results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "                results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "                results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_1:\n",
    "                result[f'hits@1_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_10:\n",
    "                result[f'hits@10_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_10[section]\n",
    "\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_100:\n",
    "                result[f'hits@100_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_10_mean = [[results_hits_10[f'{joint_section}_{subj_section}'][0] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "    hits_10_std = [[results_hits_10[f'{joint_section}_{subj_section}'][1] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "\n",
    "    data = np.array(hits_10_mean)\n",
    "\n",
    "    mask = np.ones_like(data.T, dtype='bool')\n",
    "    mask[np.triu_indices_from(mask)] = False\n",
    "    mask = np.rot90(mask, 1)\n",
    "\n",
    "    data = np.flipud(data)\n",
    "\n",
    "    ax = sns.heatmap(data, mask=mask, annot=True, fmt=\".2f\", linewidth=0.5, cmap='Blues',\n",
    "                     cbar_kws={'label': 'Hits@10'})\n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "# Rotate the tick labels for clarity\n",
    "plt.xticks(range(len(joint_freq_bins)), joint_freq_bins, rotation=0, ha='right')\n",
    "plt.yticks(range(len(subject_freq_bins)), subject_freq_bins[::-1], rotation=0)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "plt.ylabel('Subject frequency')\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'{dataset_name}_{dataset_type}_{model_name}_{training_type}_hits@10_against_condprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model_name_dict = {\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "joint_freq_bins = [f'$10^{i}$' for i in range(6+1)]\n",
    "subject_freq_bins = [f'$10^{i+1}$' for i in range(6)]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}_4_shot.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        joint_freq = subj_obj_count\n",
    "        joint_section = frequency_to_section(joint_freq)\n",
    "\n",
    "        subj_freq = subj_count\n",
    "        subj_section = frequency_to_section(subj_freq)\n",
    "\n",
    "        section = f'{joint_section}_{subj_section}'\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    joint_sections = range(1, len(bins)+1)\n",
    "    subj_sections = range(1, len(bins)+1)\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "            if section in results_hits_1:\n",
    "                results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "                results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "                results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_1:\n",
    "                result[f'hits@1_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_10:\n",
    "                result[f'hits@10_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_10[section]\n",
    "\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_100:\n",
    "                result[f'hits@100_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_10_mean = [[results_hits_10[f'{joint_section}_{subj_section}'][0] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "    hits_10_std = [[results_hits_10[f'{joint_section}_{subj_section}'][1] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "\n",
    "    data = np.array(hits_10_mean)\n",
    "\n",
    "    mask = np.ones_like(data.T, dtype='bool')\n",
    "    mask[np.triu_indices_from(mask)] = False\n",
    "    mask = np.rot90(mask, 1)\n",
    "\n",
    "    data = np.flipud(data)\n",
    "\n",
    "    ax = sns.heatmap(data, mask=mask, annot=True, fmt=\".2f\", linewidth=0.5, cmap='Blues',\n",
    "                     cbar_kws={'label': 'Hits@10'})\n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "# Rotate the tick labels for clarity\n",
    "plt.xticks(range(len(joint_freq_bins)), joint_freq_bins, rotation=0, ha='right')\n",
    "plt.yticks(range(len(subject_freq_bins)), subject_freq_bins[::-1], rotation=0)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "plt.ylabel('Subject frequency')\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'{dataset_name}_{dataset_type}_{model_name}_4shot_hits@10_against_condprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
